{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Learning Checkpoint: Vision"
      ],
      "metadata": {
        "id": "8rV5yUUpf9BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Image Embeddings\n",
        "- Binary: 0 = black, 1 = white\n",
        "- Grayscale: 0 = black <=> 255 = white\n",
        "- RGB: 3 values from 0 to 255 capturing how much red, green, and blue \n",
        "\n",
        "Knowledge Check:\n",
        "- (i) What is the tradeoff between binary and grayscale? Which side of the tradeoff should we select in today's standards? \n",
        "- (ii) How many bytes do we need to represent a 1024 x 1024 image using RGB?"
      ],
      "metadata": {
        "id": "lCuToN-vKTMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Convolutional Neural Nets\n",
        "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n",
        "* **Kernel:** size of convolution/pooling area of consideration.\n",
        "* **Padding:** adds null values to outside edges of the input.\n",
        "* **Stride:** step size of each convolution/pool. \n",
        "* **Filter:** parallel series of layers.\n",
        "* **Convolutional Layer:** uses filters that perform convolution operations as it is scanning with respect to its dimensions to capture relative context.\n",
        "* **Pooling Layer:** downsampling operation to account for spatial invariance.\n",
        "* **Fully Connected Layer:** connects a flattened output to a fully connected neural net.\n",
        "\n",
        "Knowledge Check:\n",
        "- (i) Given an image of size 64×64, what is the output dimension after applying a convolution\n",
        "with kernel size 5 × 5, stride of 2, and no padding?\n",
        "- (ii) Given an input of dimension C × H × W, what is the output dimension of a convolutional layer with kernel size K × K, padding P, stride of S, and F channels (also called filters)?\n",
        "- Suppose we have one batch 100 input images each of size 3 × 64 × 64. Consider a convolutional layer with 2 output channels, kernel size 5 × 5, no padding, and stride of 2. \n",
        "  - (iii) What is the shape of the weight parameters for the convolutional layer?\n",
        "  - (iv) What is the output size after we feed the whole batch of input images through the convolutional layer?\n",
        "  - (v) We decide to add a linear layer after the convolutional layer to make a prediction of whether the image contains a Changaroo, what would be the input dimension for the\n",
        "linear layer?"
      ],
      "metadata": {
        "id": "PJ0v4W50KXMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. CLIP\n",
        "https://arxiv.org/abs/2103.00020\n",
        "* Previous work trains an image feature extractor and a linear classifier for the image classification problem.\n",
        "* CLIP restructures the problem with the following architecture:\n",
        "  - Input: (text, image) pairs\n",
        "  - Image Encoder: ResNet with modernizations\n",
        "  - Text Encoder: Pretty standard Transformer \n",
        "  - Task: Given $N$ (text, image) pairs and $N \\times N$ possible (text, pair) combinations, maximize the cosine similarity between the text and image embedding for the true pairs, and minimize for the incorrect pairs (see Figure 1)\n",
        "* For the zero-shot inference classification setting as an example:\n",
        "  - Input: image and possible text labels\n",
        "  - Algorithm returns the label whose text embedding that has the highest cosine similarity with the image embedding\n",
        "\n",
        "Knowledge Check:\n",
        "* (i) Draw out a table of the ideal cosine similarity with the following (text, image) pairs: (^w^, happy), (-_-, annoyed), (:x, speechless)\n",
        "* (ii) How can CLIP be applied to guide open-ended text generation, say with GPT-3, where you are given the top k outputs and their associated log probabilities?"
      ],
      "metadata": {
        "id": "YXrNhrGtKZ-H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_yo0F4AorYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}