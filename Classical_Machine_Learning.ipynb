{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. ML Background\n",
        "A. Train-Validate-Test Paradigm\n",
        "- Traditionally, a dataset needs to be collected for a specific task. \n",
        "- This dataset is split into a training/validation/test set using a 80/10/10 split.\n",
        "- We fit a model's parameters using the training data. \n",
        "- Then, the model's hyperparameters are evaluated and selected using the validation set.\n",
        "- Finally, the test set is to determine the final performance. \n",
        "- This paradigm helps mitigate overfitting to the training data, preventing our model from generalizing well.\n",
        "- Let's walk through a simple example.\n",
        "\n",
        "B. Fine-Tuning\n",
        "- We previously treat a neural net as a singular black box. Nowadays, this black box is VERY BIG. \n",
        "- These BIG black boxes tend to perform decently well on a variety of tasks, even without fitting a model to a specific task. \n",
        "- Yet, these models can perform even better if we add a single additional layer at the beginning of the neural net, which we \"fine-tune\" to our task of interest. (Adding a smaller black box before the bigger black box.) \n",
        "- This allows us to significantly reduce computing resources.\n",
        "\n",
        "C. Few Shot/Zero Shot Settings\n",
        "- With the new models like GPT-3 and OPT containing 175B parameters, these black boxes are EVEN BIGGER.\n",
        "- It turns out, these models perform super well on tasks, even without fitting or fine-tuning a model to that task.\n",
        "- In the few shot setting, we provide the model of one or few examples, and ask the model to complete the task.\n",
        "- In the zero shot setting, we ask the model to complete the task without seeing any training examples specific to the task."
      ],
      "metadata": {
        "id": "aeBlLYa5uLMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Learning Checkpoint: Classical ML\n",
        "A. Linear Regression\n",
        "- Given: input data $(x_1, t_1), ..., (x_n, t_n)$ where $x \\in R^m$ and target $t \\in R^1$.\n",
        "- Goal: fit an order $M$ polynomial function that is linear with respect to weights $w \\in R^{m+1}$:\n",
        "$$y(x, w) = w_0 + w_1 x + ... + w_m x^M$$\n",
        "- To optimize, we need to find an optimal $w^*$ that minimizes the least squares loss function:\n",
        "$$E(w) = \\frac{1}{2} \\sum_i^n (y(x_i, w) - t_i)^2 = \\frac{1}{2} ||Xw-t||_2^2$$\n",
        "- To minimize, we move in the direction of steepest descent (negative gradient):\n",
        "$$-\\nabla_w E(w) = X^T t - X^T X w$$\n",
        "- At inference time, we simply use our linear model:\n",
        "$$\\hat{y} = Xw^*$$"
      ],
      "metadata": {
        "id": "0QLyVYcGEnUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UZmHQX_4ohx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 60\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "im5VTw0TokX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy dataset\n",
        "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
        "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
        "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
        "\n",
        "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
        "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
        "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
      ],
      "metadata": {
        "id": "m4ALHB4toovI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression model\n",
        "model = nn.Linear(input_size, output_size, bias=True)"
      ],
      "metadata": {
        "id": "fr8mBO-Horbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the dataset\n",
        "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
        "plt.plot(x_train, y_train, 'ro', slabel='Original data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bBFAW9Ero-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "metadata": {
        "id": "trCXiZj2oyEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    inputs = torch.from_numpy(x_train)\n",
        "    targets = torch.from_numpy(y_train)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "id": "0cP38NYska1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the graph\n",
        "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
        "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
        "plt.plot(x_train, predicted, label='Fitted line')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3oJrw3Fo6LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Logistic Classifier\n",
        "- Derived from our linear model, we now map the real value output to a binary classification. Specifically, we use sigmoid activation:\n",
        "$$\\theta(s) = \\frac{\\exp(s)}{1 + \\exp(s)} = \\frac{1}{1 + \\exp(-s)}$$\n",
        "- We define classifier as follows, with default decision boundary = 0.5:\n",
        "$$h(x) = \\theta(w^T x)$$\n",
        "- Once again, we want to find optimal $w^*$, done using (stochastic) gradient descent, moving in the direction of steepest descent. For practicality purposes, we abstract the math.\n"
      ],
      "metadata": {
        "id": "qabq5LkAlJON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "nk_giywWq4KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters \n",
        "input_size = 28 * 28    # 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "jm9KfcGHq6X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "LQErHNBqq8Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "pf7FBmoprDIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)"
      ],
      "metadata": {
        "id": "sUGA03mprFPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "metadata": {
        "id": "fVerOWWCrHLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "id": "AzIsluarpKp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "oTtxtGTGrLsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Feed Forward Neural Net\n",
        "- Think of this as a nested logistic classifier.\n",
        "- If a hidden layer has $n$ nodes, we have $n$ independent logistic classifiers, each with a different set of weights.\n",
        "- To get the final prediction, we have another logistic classifier, taking the outputs of the hidden layer (for a single hidden layer network).\n",
        "- Let's draw this out.\n",
        "- This is powerful because the sigmoid activation is a nonlinear function that allows us to expand our modelling space beyond linear transformations."
      ],
      "metadata": {
        "id": "Rr5lJfa9pNpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "qPzCnpRYr2h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "eYT-dFsar5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters \n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "iBQrULuir6wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "hfgqKI7Cr8TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "L90rGspkr9ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "dIa-HCIBsCfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
      ],
      "metadata": {
        "id": "ykOiZzRosEI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
      ],
      "metadata": {
        "id": "xPpU0esOsG1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "id": "8mD_Q361rO9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "UKnkMubbsLjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Knowledge Check \n",
        "  - Linear Regression\n",
        "    - What is the task we are trying to accomplish?\n",
        "    - What is the training objective that aims to solve this task?\n",
        "    - How do we use the training objective to find an optimal solution (in the convex case)?\n",
        "  - Logistic Classifier\n",
        "    - How does the logistic classifier relate to linear regression?\n",
        "    - How do we achieve a binary output?\n",
        "  - Feed Forward Neural Net\n",
        "    - How does the feed forward neural net relate to the logistic classifier?\n",
        "    - Why do we use nonlinear activation transformation functions in FFNNs?\n",
        "  - Coding Abstractions \n",
        "    - What is a data loader?\n",
        "    - What are the forward and backward steps?"
      ],
      "metadata": {
        "id": "7wYYqRQqRqHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KRsZqqn9PHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}