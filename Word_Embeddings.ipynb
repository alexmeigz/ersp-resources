{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Learning Checkpoint: Word Embeddings\n",
        "- Reference: https://sites.cs.ucsb.edu/~alexmei/documents/presentations/word_embeddings.pdf"
      ],
      "metadata": {
        "id": "Wswpxwh3zc69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Preliminaries\n",
        "* Word Embeddings: translations of text into alternative representations\n",
        "* Why? alternative representations are easier to manipulate and quantify\n",
        "- Assume Corpus (C) is the set of unique words in vocabulary under consideration\n",
        "- **Bag of Words**: vector of length C with each element i denoting count of word i\n",
        "- **One Hot Vector**: instead of count of word i, use 1 if word in text else 0\n",
        "\n",
        "Checkpoint:\n",
        "- Let `d = {0: \"Alex\", 1: \"William\", 2: \"machine(s)\", 3: \"learning\", 4: \"student\", 5: \"professor\"}` where the `v[i]` is the representation of the word `d[i]`. \n",
        "- (i) What is the bag of words representation for \"Alex is a machine learning student that likes machines\"? Denote this as $v_1$.\n",
        "- (ii) What is the one hot vector representation for \"William is a machine learning professor that likes to machines\"? Denote this as $v_2$.\n",
        "- (iii) What is a limitation of the bag of words / one hot vector representation?"
      ],
      "metadata": {
        "id": "1ReEsvQvQkmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "v1 = np.array([1, 0, 1, 1, 1, 0])\n",
        "v2 = np.array([0, 1, 1, 1, 0, 1])"
      ],
      "metadata": {
        "id": "pditqcJ-aghk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Similarity\n",
        "* Objective: given two vectors, $v_i, v_j$, compare their similarity.\n",
        "* Example Task: plagiarism checker for text similarity\n",
        "* **Euclidean Distance**: $$||v_i - v_j||_2$$\n",
        "* **Dot Product**: $$v_i^T v_j$$\n",
        "* **Cosine Similarity**: $$\\frac{v_i^T v_j}{||v_i||_2 ||v_j||_2}$$\n",
        "\n",
        "Checkpoint:\n",
        "* (i) What is the simiarity of $v_1, v_2$ using Euclidean Distance?\n",
        "* (ii) What is the similarity of $v_1, v_2$ using Dot Product?\n",
        "* (iii) What is the similarity of $v_1, v_2$ using Cosine Similarity?\n",
        "* (iv) What is a disadvantage of the euclidean distance metric? (Hint: think about dimensionality.)\n",
        "* (v) What is a disadvantage of the dot product metric? (Hint: think about range.)\n",
        "* (vi) What is a limitation of the cosine similarity metric? (Hint: think about domain.)"
      ],
      "metadata": {
        "id": "zwqX-Vk9WXr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy.linalg as npla\n",
        "\n",
        "print(\"Euclidean Distance:\", npla.norm(v1 - v2, 2))\n",
        "print(\"Dot Product:\", v1.T @ v2)\n",
        "print(\"Cosine Similarity:\", v1.T @ v2 / (npla.norm(v1, 2) * npla.norm(v2, 2)))"
      ],
      "metadata": {
        "id": "Vf3RFY5CaluE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Context\n",
        "* Objective: make better representations by adding context to the word representation.\n",
        "* Example: consider words with multiple meanings, such as 'rob a rich bank' vs 'sleep by the river bank'.\n",
        "* **N-Gram**: instead of using a single word as a token in the corpus, use a window of n words as the token.\n",
        "* **Window-Based**: define a matrix A that is C x C. Let A[i][j] = 1 if d[j] is in the context window of size n of d[i]. \n",
        "\n",
        "Checkpoint:\n",
        "* Consider the sentence \"Ryan likes to play Genshin Impact\". \n",
        "* (i) What are the 2-grams for this sentence?\n",
        "* (ii) How many possible 3-grams are in this sentence?\n",
        "* (iii) What is a limitation of this N-Gram model? (Hint: think what happens as n increases.)\n",
        "* Now, consider the sentence \"Vaishnavi loves all things sugar\" with d = {0: \"Vaishnavi\", 1: \"loves\", 2: \"all\", 3: \"things\", 4: \"sugar\"}.\n",
        "* (iv) What is the window-based representation of this sentence with context window size 2?\n",
        "* (v) What is a limitation of this Window-Based model?\n",
        "* (vi) What is a potential solution to the limitation of the Window-Based model? "
      ],
      "metadata": {
        "id": "1OrOOzawZDQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_extractor(word_list: list, n: int) -> list:\n",
        "  return [\" \".join(word_list[i: i+n]) for i in range(len(word_list) - n + 1)]\n",
        "\n",
        "print(\"2-Grams:\", ngram_extractor(\"Ryan likes to play Genshin Impact\".split(\" \"), 2))\n",
        "print(\"Number of 3-Grams:\", len(ngram_extractor(\"Ryan likes to play Genshin Impact\".split(\" \"), 3)))"
      ],
      "metadata": {
        "id": "nWORQScibYuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_extractor(word_list: list, corpus: dict, n: int) -> list:\n",
        "  length = len(word_list)\n",
        "  A = [[0] * length for _ in word_list]\n",
        "\n",
        "  for i in range(length):\n",
        "    for j in range(length):\n",
        "      if i - n < j < i + n and i != j:\n",
        "        A[corpus[word_list[i]]][corpus[word_list[j]]] = 1\n",
        "\n",
        "  return A\n",
        "\n",
        "print(\"Size 2 Window:\", window_extractor(\"Vaishnavi loves all things sugar\".split(\" \"), {\"Vaishnavi\": 0, \"loves\": 1, \"all\": 2, \"things\": 3, \"sugar\": 4}, 2))"
      ],
      "metadata": {
        "id": "20Yd8r-XQuVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Learned Representations\n",
        "* Machine learning does well on a variety of tasks. What if we just train a NN to learn a dense representation?\n",
        "* **Continuous Bag of Words**: predict target word given context word.\n",
        "  * Input: $1 \\times C$ representation of context word (i.e., one-hot).\n",
        "  * Hidden Layer of size $M$\n",
        "  * Output $1 \\times V$ dense representation of target word.  \n",
        "* **Skip gram**: predict context words given target word.\n",
        "  * Input: $1 \\times C$ representation of target word (i.e., one-hot).\n",
        "  * Hidden Layer of size $M$\n",
        "  * Output $1 \\times V$ dense representation of context word.  \n",
        "* **Softmax Loss Function**: interested in class $i$, given $K$ classes,\n",
        "$$\\sigma(v_i) = \\frac{\\exp(v_i)}{\\sum_k \\exp(v_k)}$$\n",
        "\n",
        "For the implementation, refer to https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28 \n",
        "\n",
        "Checkpoint:\n",
        "* (i) What type of learning problem is CBOW/Skipgram? (i.e., Supervised, Unsupervised, Self-Supervised, Weakly Supervised)\n",
        "* (ii) Draw the neural-network that represents the CBOW/Skipgram architecture.\n",
        "* (iii) How does the softmax function relate to the sigmoid function?\n",
        "* (iv) How many learnable parameters does the CBOW/Skipgram architecture have? (Don't forget to include bias.)\n",
        "* (v) What is a potential limitation of the CBOW/Skipgram models? \n",
        "* (vi) What is a potential solution to this limitation?"
      ],
      "metadata": {
        "id": "_S9ypNTGPRNj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_yo0F4AorYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}