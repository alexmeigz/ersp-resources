{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Learning Checkpoint: Transformers"
      ],
      "metadata": {
        "id": "bm_gok6ylscR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I. Recap\n",
        "- Recall that a limitation of RNNs is the lack of long range dependencies.\n",
        "- LSTMs aim to solve the problem with specific gates, but the context is not explicitly modeled.\n",
        "- In practice, RNNs/LSTMs face the vanishing gradient issue, where long range context eventually result in negligible gradients.\n",
        "- In addition, the variable length input makes the computation impossible to parallelize."
      ],
      "metadata": {
        "id": "oso05StcbXEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### II. Attention Model for Embeddings\n",
        "- Original Attention Paper: https://arxiv.org/pdf/1409.0473.pdf\n",
        "- Attention for Transformers: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "\n",
        "- To simulate variable length inputs, we simply use an <empty> token to pad out the input up to a token limit.\n",
        "- $$A(q, K, V) = \\sum_i \\frac{ \\exp(q^T k_i) }{ \\sum_j \\exp(q^T k_j) } v_i = softmax(Q K^T) V$$\n",
        "    - $q_i$ is the ith input embedding\n",
        "    - $k_j, v_j$ are key, value pairs that represent hidden state representations, which serve as candidates; specifically, $v_j$ is the jth possible output, which is produced using key $k_j$. \n",
        "- The interpretation of the attention output is that it returns a linear combination of the candidate values.\n",
        "- Note the dimensionality of the model; $q_i$ and $k_j$ must be the same dimensions for the dot product to work; $v_j$ can be a different dimension.\n",
        "    - Assume an input size $M$ and the set of candidates is size $C$. If $q, k$ is of dimension $d_q$ and $v$ is of dimension $d_v$, then the output dimension is $[M \\times d_v]$\n",
        "- Observe that when $d_q$ gets larger, the variance of $d_q$ increases, which increases the softmax and makes the gradient very small. We must scale our attention model down by a factor of $\\sqrt{d_q}$.\n",
        "- $$A = softmax(\\frac{Q K^T}{\\sqrt{d_q}}) V$$\n",
        "\n",
        "Knowledge Check:\n",
        "- (i) Show the individual operations of the attention model such that the output dimension of the scaled dot-product attention model has an output of $[M \\times d_v]$\n",
        "- (ii) Explain why the attention model helps improve parallelism over RNNs/LSTMs. (Hint: think about the autoregressive property.)\n",
        "- (iii) Explain the attention model in the lens of global vs local context and how this relates to the story generation step in our project."
      ],
      "metadata": {
        "id": "q_nsWJRblty7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### III. Self-Attention\n",
        "- Instead of using an annotated set of key, value candidates, use the set of keys as all the other input embeddings.\n",
        "- By using other inputs, we can attend to the relevant parts of the input without a range restriction for the best context to generate an optimal candidate; this is controlled via the dot product."
      ],
      "metadata": {
        "id": "NgfFCruRbb9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IV. Multi-Head Attention\n",
        "- Recall the concept of convolutions for CNNs; Multi-Head attention does the same thing: we project onto different heads, and then concatenate the result as the output.\n",
        "- $$MHA(Q, K, V) = Concat(head_1, ..., head_h) W^O$$\n",
        "- $$head_i = A(Q W_i^Q, K, W_i^K, V W_i^V)$$\n",
        "    - $d_{in}$ is the input embedding size; previously $d_{in} = d_q$, but now $d_q = d_{in} / h$\n",
        "    - $d_{out}$ is the output embedding size; previously $d_{out} = d_v$, but not $d_v = d_{out} / h$\n",
        "\n",
        "Knowledge Check: \n",
        "* (i) Show that the weight matrices of the Multi-Head Attention Model are the following dimensions to get the final model output of $[M \\times d_{out}]$:\n",
        "  - $W_i^Q = [d_{in} \\times d_q]$\n",
        "  - $W_i^K = [d_{in} \\times d_q]$\n",
        "  - $W_i^V = [d_{out} \\times d_v]$\n",
        "  - $W_i^O = [h * d_v \\times d_{out}]$\n"
      ],
      "metadata": {
        "id": "8iiWrDoPbgWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V. Encoders and Decoders\n",
        "- An encoder translates the input to a hidden state. At encoding time, you can use both past and present context because you are given the full input.\n",
        "- A decoder translates the hidden state to an output. At decoding time, you can only use the past context because the future context has not been generated yet. \n",
        "- Applying the attention model, an encoder self-attention model can use what we discussed. However, the decoder cannot see past the generated token, so we must mask those tokens with 0. "
      ],
      "metadata": {
        "id": "oNz-3ZBpbils"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VI. Transformers\n",
        "- Additional Components\n",
        "    - Residual Connection: addition of input without attention\n",
        "    - Layer Normalization: change input to have 0 mean and unit variance per each layer per each training point\n",
        "    - Positional embeddings, as we discussed before, to encode positions using an oscillating function; formula not important\n",
        "\n",
        "- Encoder Block\n",
        "    - Multihead Attention \n",
        "    - Add and Norm\n",
        "    - Feedforward Neural Net\n",
        "    - Add and Norm\n",
        "- Decoder Block\n",
        "    - Masked Multi-Head Partial Attention\n",
        "    - Add and Norm\n",
        "    - Multihead Attention\n",
        "    - Add and Norm\n",
        "    - Feedforward Neural Net\n",
        "    - Add and Norm"
      ],
      "metadata": {
        "id": "ojyo4-xdl7um"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_yo0F4AorYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}